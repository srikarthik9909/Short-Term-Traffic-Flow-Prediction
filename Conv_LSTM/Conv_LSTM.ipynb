{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNxAzskqWQ6R"
      },
      "source": [
        "# What is Convolutional LSTM Model\n",
        "\n",
        "\n",
        "*   Convolutional LSTM is a neural network that combines the Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models to handle spatiotemporal data.\n",
        "*   CNN used to capture the spatial dependencies in traffic flow\n",
        "data.\n",
        "*   LSTM used to capture short-term changes and periodic patterns in\n",
        "traffic flow. The LSTM structure helps in handling long-term\n",
        "dependencies. Includes daily and\n",
        "weekly periodic features using LSTMs to capture long-term\n",
        "and short-term variations.\n",
        "*   Feature-Level Fusion is combines features from CNN and LSTM and helps improve performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTlcI6zkTMI"
      },
      "source": [
        "# Necessary Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwfTnNOQkdmh"
      },
      "source": [
        "Imports all required libraries for data processing, model building, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xPGaGDGehex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model ,save_model,load_model\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Flatten, concatenate, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUgHEkNlkgBe"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHTHtpvtkjgj"
      },
      "source": [
        "Loads CSV files from different folders, processes them, and combines them into a single DataFrame, ensuring no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1p0_X6jkStv"
      },
      "outputs": [],
      "source": [
        "folder_paths = [\"Station - 1\",\"Station - 2\",\"Station - 3\",\"Station - 4\"]\n",
        "dfs_per_folder = []\n",
        "for idx, folder_path in enumerate(folder_paths, start=1):\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "    files = [f for f in files if f != 'combined.csv']\n",
        "    files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n",
        "    dfs = []\n",
        "    for file in files:\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        df = pd.read_csv(file_path)\n",
        "        df.columns = [f\"{col}_{idx}\" for col in df.columns]\n",
        "        dfs.append(df)\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "    dfs_per_folder.append(combined_df)\n",
        "final_combined_df = pd.concat(dfs_per_folder, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjzN75UClGbW"
      },
      "outputs": [],
      "source": [
        "final_combined_df = final_combined_df.iloc[:-3]\n",
        "df = pd.DataFrame(final_combined_df)\n",
        "final_combined_df = df.dropna()\n",
        "final_combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVq6fovlGI4"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBAESmalCEA"
      },
      "source": [
        "Scales the target columns using MinMaxScaler to bring data into a range between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWphoHQqlRNb"
      },
      "outputs": [],
      "source": [
        "our_particular_station = ['Station - 3']\n",
        "station_scaler = MinMaxScaler()\n",
        "final_combined_df[our_particular_station] = station_scaler.fit_transform(final_combined_df[our_particular_station])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbq-q6Sclh4c"
      },
      "outputs": [],
      "source": [
        "flow_columns = ['5 Minutes_1', \"Station - 1\",\"Station - 2\",\"Station - 3\",\"Station - 4\"]\n",
        "flows_df = final_combined_df[flow_columns].copy()\n",
        "flows_df.rename(columns={'5 Minutes_1': '5 Minutes'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vgsrpsOltes"
      },
      "source": [
        "# Sequence creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_RNsxqTlvdN"
      },
      "source": [
        "Converts the time series data into sequences for CNN and LSTM, allowing models to learn patterns over time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQodIOHNlytH"
      },
      "outputs": [],
      "source": [
        "def create_sequences(column_data, time_steps):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(column_data) - time_steps):\n",
        "        X.append(column_data[i:i + time_steps])\n",
        "        Y.append(column_data[i + time_steps])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "column_data = flows_df[[\"Station - 1\",\"Station - 2\",\"Station - 3\",\"Station - 4\"]].values\n",
        "time_steps = 15\n",
        "\n",
        "cnn_X, _ = create_sequences(column_data, time_steps)\n",
        "\n",
        "station1_data = column_data[:, 0].reshape(-1, 1)\n",
        "lstm_X, y = create_sequences(station1_data, time_steps)\n",
        "\n",
        "print(f\"Values of cnn_X: {cnn_X}\")\n",
        "print(f\"Values of y: {y}\")\n",
        "\n",
        "print(f\"Shape of cnn_X: {cnn_X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHPZ4gLol79-"
      },
      "source": [
        "# Data Spliting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_24f0n4l8rG"
      },
      "source": [
        "Splits the data into training and testing sets for both CNN and LSTM inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXCTEkVJmEKK"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(cnn_X) * 0.85)\n",
        "test_size = len(cnn_X) - train_size\n",
        "\n",
        "X_train_cnn, X_test_cnn = cnn_X[:train_size], cnn_X[train_size:]\n",
        "X_train_lstm, X_test_lstm = lstm_X[:train_size], lstm_X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"Training data size: {train_size}, Testing data size: {test_size}\")\n",
        "\n",
        "print(\"Shapes of the data:\")\n",
        "print(f\"X_train_cnn shape: {X_train_cnn.shape}\")\n",
        "print(f\"X_test_cnn shape: {X_test_cnn.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"Training CNN X Values : \",X_train_cnn)\n",
        "print(\"Testing CNN X Values : \",X_test_cnn)\n",
        "print(\"Training y Values : \",y_train)\n",
        "print(\"Testing y Values : \",y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXURHmLcmJ0d"
      },
      "source": [
        "# CONV-LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkKRIibmJbC"
      },
      "source": [
        "Defines a Convolutional LSTM model that combines CNN for spatial features and LSTM for temporal features, with a final dense layer for predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG5KHz_6mJJm"
      },
      "outputs": [],
      "source": [
        "cnn_input = Input(shape=(15, 4))\n",
        "\n",
        "conv1 = Conv1D(filters=32,kernel_size=8,strides=1,activation='tanh', padding='same')(cnn_input) # Add padding='same'\n",
        "pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
        "\n",
        "conv2 = Conv1D(filters=16,kernel_size=4,strides=1,activation='tanh', padding='same')(pool1) # Add padding='same'\n",
        "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "\n",
        "lstm1 = LSTM(100, return_sequences=True)(pool2)\n",
        "lstm2 = LSTM(50)(lstm1)\n",
        "\n",
        "output_layer = Dense(1)(lstm2)\n",
        "\n",
        "model = Model(inputs=cnn_input, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss='mean_squared_error')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGQy9KyVmXvG"
      },
      "source": [
        "# Training the CONV-LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYwb4T6jmXqR"
      },
      "source": [
        "Trains the model on the training data for 100 epochs with a validation split of 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dWt5w5DmXZd"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_cnn, y_train, epochs=100, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFzbOnPvmkks"
      },
      "source": [
        "# Predictions and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfMVQBFkmkhQ"
      },
      "source": [
        "Makes predictions on the test set and rescales them to their original values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQv7La5EmqTB"
      },
      "outputs": [],
      "source": [
        "predicted_values = model.predict(X_test_cnn)\n",
        "predictions_rescaled = station_scaler.inverse_transform(predicted_values.reshape(-1, 1)).flatten()\n",
        "y_test_rescaled = station_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "results = pd.DataFrame(data={'Predictions': predictions_rescaled, 'Actuals': y_test_rescaled})\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUfCBTRamyPZ"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUvJH9UvmyIk"
      },
      "source": [
        "Calculates RMSE, MAE, and MAPE to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uD43_CCm22r"
      },
      "outputs": [],
      "source": [
        "np.isnan(predictions_rescaled).any()\n",
        "np.isnan(y_test_rescaled).any()\n",
        "\n",
        "predictions_rescaled = np.nan_to_num(predictions_rescaled, nan=np.nanmean(predictions_rescaled))\n",
        "y_test_rescaled = np.nan_to_num(y_test_rescaled, nan=np.nanmean(y_test_rescaled))\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(predictions_rescaled,y_test_rescaled))\n",
        "mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
        "mape = mean_absolute_percentage_error(predictions_rescaled, y_test_rescaled)\n",
        "print (\"RMSE:\", rmse)\n",
        "print (\"MAE:\", mae)\n",
        "print (\"MAPE:\", mape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC0j--tpm9C4"
      },
      "source": [
        "# Save and Load the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pUSFt-8m83x"
      },
      "source": [
        "Saves the trained model to a file and demonstrates how to load it for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_zlVzs2nFBU"
      },
      "outputs": [],
      "source": [
        "model.save('CONV-LSTM-Model.h5')\n",
        "loaded_model = load_model('CONV-LSTM-Model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Jt94UsnQ_V"
      },
      "source": [
        "# Loss Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRUp5MarnQo8"
      },
      "source": [
        "Plots the training and validation loss over epochs to visualize model convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxeNx8hNnRui"
      },
      "outputs": [],
      "source": [
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
